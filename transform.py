# -*- coding: utf-8 -*-
"""transform.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ILRd4lrwUOA5HuP3wA3euAuR82V2PP7h
"""

import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK data (run only once)
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')  # Needed for word_tokenize

# Cleaning Column Names
def clean_column_names(df):
    # Formatting column names by changing to lowercase and replacing any spaces to underscores for consistency
    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')
    return df

# Drop Duplicated Column
def drop_duplicated_column(df):
    # Drop the original column
    df.drop(columns=['comfort_food_reasons_coded'], inplace=True)
    # Rename the duplicate, which originally had a suffix
    df.rename(columns={'comfort_food_reasons_coded.1': 'comfort_food_reasons_coded'}, inplace=True)
    return df

# Clean Float Columns (GPA and Weight)
def clean_float_columns(df):
# Function to extract numeric values from strings
    def extract_numeric(value):
        try:
            # Use regex to find the first decimal number in the string
            return float(re.findall(r'\d+\.\d+|\d+', str(value))[0])
        except (IndexError, ValueError):
            return None

    # Apply cleaning
    df['gpa'] = df['gpa'].apply(extract_numeric)
    df['weight'] = df['weight'].apply(extract_numeric)

    # Convert to float explicitly
    df['gpa'] = df['gpa'].astype(float)
    df['weight'] = df['weight'].astype(float)
    return df

# Handling Missing Values
def handle_missing_values(df):
    open_ended_cols = [
    'comfort_food', 'comfort_food_reasons', 'diet_current',
    'eating_changes', 'father_profession', 'fav_cuisine',
    'food_childhood', 'healthy_meal', 'ideal_diet',
    'meals_dinner_friend', 'mother_profession', 'type_sports'
    ]

    skip_cols = open_ended_cols + ['gpa', 'weight']

    special_cols = ['calories_day', 'calories_scone', 'employment']

    # Impute the 3 columns with the inferred values

    # Replace missing values in 'calories_day' with '1'
    df['calories_day'] = df['calories_day'].fillna(1)

    # Replace missing values in 'calories_scone' with '107'
    df['calories_scone'] = df['calories_scone'].fillna(107)

    # Replace missing values in 'employment' with '1'
    df['employment'] = df['employment'].fillna(4)

    # Second Round of Cleaning

    # 1. Impute 'gpa' and 'weight' with median
    df['gpa'] = df['gpa'].fillna(df['gpa'].median())
    df['weight'] = df['weight'].fillna(df['weight'].median())

    # 2. Define columns to skip for mode imputation (This is essentially all other columns which are Categorical)
    skip_cols = open_ended_cols + special_cols + ['gpa', 'weight']

    # 3. Impute all other columns (except skipped) with mode
    mode_impute_cols = [col for col in df.columns if col not in skip_cols]

    for col in mode_impute_cols:
        df[col] = df[col].fillna(df[col].mode()[0])

    # 4. Impute open-ended columns with "not specified"
    for col in open_ended_cols:
        df[col] = df[col].fillna("not specified")

    return df

# Cleaning Open-ended Columns/Questions for NLP Purposes
def clean_open_ended_columns(df):
    open_ended_cols = [
    'comfort_food', 'comfort_food_reasons', 'diet_current',
    'eating_changes', 'father_profession', 'fav_cuisine',
    'food_childhood', 'healthy_meal', 'ideal_diet',
    'meals_dinner_friend', 'mother_profession', 'type_sports'
    ]

    # Defining the main cleaning text for NLP preparations
    def clean_text(text):
        text = str(text).lower()
        text = re.sub(r'[^a-z\s]', '', text)  # remove punctuation and special characters
        text = re.sub(r'\s+', ' ', text).strip()  # normalize whitespace
        return text

    # Defining Cleaning Function for Stop Words Removal & Lemmatization
    # In our case, since the data mining goal is not defined, we will simply assume that it may need stop words removal and lemmatization for future NLP analysis
    # In console: !pip install nltk
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    def lemmatize(text):
        tokens = text.split()
        filtered = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]
        return ' '.join(filtered)

    for col in open_ended_cols:
        df[col] = df[col].apply(clean_text)
        #Uncomment the line below if analysis REQUIRES Lemmatization & Stop Words Removal.
        #df[col] = df[col].apply(lemmatize)

    # Drop rows where any open-ended column became empty
    df = df[~df[open_ended_cols].apply(lambda row: row.str.strip().eq('').any(), axis=1)]
    return df

# Removing Outliers for GPA and Weight
def remove_outliers(df):
    # Finding the lower and upper bounds of 'gpa'
    IQR_Gpa = df["gpa"].quantile(0.75) - df["gpa"].quantile(0.25)

    # UpperBound_Gpa = df["gpa"].quantile(0.75) + 1.5*IQR_Gpa # There is no gpa greater than 4 so we clip it to 4.0
    UpperBound_Gpa = 4.0
    LowerBound_Gpa = df["gpa"].quantile(0.25) - 1.5*IQR_Gpa

    # Drop the columns that fall outside the bounds.
    df = df[df["gpa"] <= UpperBound_Gpa][(df["gpa"] >= LowerBound_Gpa)]

    # Finding the lower and upper bounds of 'weight'
    IQR_Weight = df["weight"].quantile(0.75) - df["weight"].quantile(0.25)
    UpperBound_Weight = df["weight"].quantile(0.75) + 1.5*IQR_Weight
    LowerBound_Weight = df["weight"].quantile(0.25) - 1.5*IQR_Weight

    # Drop the entries that fall outside the bounds.
    df = df[df["weight"] <= UpperBound_Weight][(df["weight"] >= LowerBound_Weight)]
    return df

# Standardization/Normalization of Float Columns
def standardize_float_columns(df):
    # Perform Max Scaling for GPA since we know the max value (4.0)
    df['gpa'] = df['gpa'] / 4.0
    # Perform Min-Max Scaling for Weight
    df["weight"] = (df["weight"] - df["weight"].min()) / (df["weight"].max() - df["weight"].min())
    return df

# Standardization of Favorite Cuisine Column
def standardized_cuisine(df):
    cuisines = ['italian', 'mexican', 'american', 'chinese', 'indian', 'asian', 'thai', 'korean', 'french', 'turkish', 'jamaican', 'spanish',
                'greek', 'japanese', 'colombian', 'vietnamese', 'lebanese', 'nepali', 'hispanic', 'german', 'seafood']
    keyword = r'\b(' + '|'.join(cuisines) + r')\b'

    def find_cuisines_regex(text):
        text = str(text).lower()
        # Explicit negative/no preference phrases
        if any(phrase in text for phrase in ['do not like', 'none', 'not specified', 'no preference']):
            return 'unspecified'

        found = re.findall(keyword, text)

        if not found:
            # If cuisine found, return 'other'
            return 'other'
        elif len(found) == 1:
            return found[0]
        else:
            # If multiple cuisines, sorted, joined by '-'
            return '-'.join(sorted(list(set(found))))

    df['fav_cuisine'] = df['fav_cuisine'].apply(find_cuisines_regex)
    return df